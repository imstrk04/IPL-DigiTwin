{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d7c922-c874-4bd7-a209-6ed92200c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAG system...\n",
      "The context retrieved is:  ['Season: 2010, Date: 13/04/10 or 13 of april 10, City: Chennai, Match: Kolkata Knight Riders vs Chennai Super Kings, Toss Winner: Kolkata Knight Riders, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Chennai Super Kings, Win By Runs: 0, Win By Wickets: 9, Player of the Match: R Ashwin, Venue: MA Chidambaram Stadium, Chepauk, Umpire1: SS Hazare, Umpire2: SJA Taufel', 'Season: 2008, Date: 26/04/08 or 26 of april 08, City: Chennai, Match: Kolkata Knight Riders vs Chennai Super Kings, Toss Winner: Kolkata Knight Riders, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Chennai Super Kings, Win By Runs: 0, Win By Wickets: 9, Player of the Match: JDP Oram, Venue: MA Chidambaram Stadium, Chepauk, Umpire1: BF Bowden, Umpire2: AV Jayaprakash', 'Season: 2008, Date: 18/05/08 or 18 of may 08, City: Kolkata, Match: Kolkata Knight Riders vs Chennai Super Kings, Toss Winner: Kolkata Knight Riders, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 1, Winner: Chennai Super Kings, Win By Runs: 3, Win By Wickets: 0, Player of the Match: M Ntini, Venue: Eden Gardens, Umpire1: Asad Rauf, Umpire2: K Hariharan', 'Season: 2010, Date: 16/03/10 or 16 of march 10, City: Kolkata, Match: Chennai Super Kings vs Kolkata Knight Riders, Toss Winner: Chennai Super Kings, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Chennai Super Kings, Win By Runs: 55, Win By Wickets: 0, Player of the Match: MS Dhoni, Venue: Eden Gardens, Umpire1: HDPK Dharmasena, Umpire2: AM Saheba', 'Season: 2009, Date: 18/05/09 or 18 of may 09, City: Centurion, Match: Chennai Super Kings vs Kolkata Knight Riders, Toss Winner: Chennai Super Kings, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Kolkata Knight Riders, Win By Runs: 0, Win By Wickets: 7, Player of the Match: BJ Hodge, Venue: SuperSport Park, Umpire1: SJA Taufel, Umpire2: RB Tiffin']\n",
      "RAG Answer: In the year 2010, Kolkata Knight Riders played against Chennai Super Kings twice. The first match took place at MA Chidambaram Stadium located in Chepauk city of Chennai on April 13/10 with Umpires SS Hazare and SJA Taufel officiating the game, which Kolkata Knight Riders won by bat batting method after Duckworth-Lewis-Stern applied. The second match was played at Eden Gardens in Kolkata on March 16/10 with Umpire HDPK Dharmasena and AM Saheba officiating the game, where Chennai Super Kings emerged victorious by bat batting method after again no Duckworth-Lewis-Stern applied.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import json\n",
    "ipl_data = pd.read_csv(\"OneDrive/Desktop/matches.csv\")\n",
    "\n",
    "month = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "months = {}\n",
    "\n",
    "for i in range(len(month)):\n",
    "    months[i+1] = month[i]\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "ipl_data['context'] = ipl_data.apply(\n",
    "    lambda x: f\"Season: {x['season']}, Date: {x['date']} or {x['date'].split('/')[0]} of {months[int(x['date'].split('/')[1])]} {x['date'].split('/')[2]}, City: {x['city']}, Match: {x['team1']} vs {x['team2']}, \"\n",
    "              f\"Toss Winner: {x['toss_winner']}, Decision: {x['toss_decision']}, Result: {x['result']}, Duckworth-Lewis-Stern applied: {x['dl_applied']}, \"\n",
    "              f\"Winner: {x['winner']}, Win By Runs: {x['win_by_runs']}, Win By Wickets: {x['win_by_wickets']}, Player of the Match: {x['player_of_match']}, Venue: {x['venue']}, Umpire1: {x['umpire1']}, Umpire2: {x['umpire2']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "tfidf_matrix = model.encode(ipl_data['context'])\n",
    "\n",
    "def retrieve_context(question, k=5):\n",
    "    question_vec = model.encode([question])\n",
    "    scores = cosine_similarity(question_vec, tfidf_matrix).flatten()\n",
    "    top_indices = scores.argsort()[-k:][::-1]\n",
    "    return ipl_data.iloc[top_indices]['context'].tolist()\n",
    "\n",
    "# Function to query Phi3 via Ollama API\n",
    "# def query_phi3(prompt):\n",
    "#     url = \"http://localhost:11434/api/generate\"  # Replace with your server URL if using ngrok\n",
    "#     headers = {\"Content-Type\": \"application/json\"}\n",
    "#     payload = {\"model\": \"phi3\", \"prompt\": prompt}\n",
    "\n",
    "#     response = requests.post(url, json=payload, headers=headers)\n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()[\"response\"]\n",
    "#     else:\n",
    "#         return \"Error querying Phi3 model.\"\n",
    "\n",
    "# def query_phi3(prompt):\n",
    "#     url = \"http://localhost:11434/api/generate/\"  # Replace with ngrok URL if applicable\n",
    "#     headers = {\"Content-Type\": \"application/json\"}\n",
    "#     payload = {\"model\": \"phi3\", \"prompt\": prompt}\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(url, json=payload, headers=headers)\n",
    "#         response.raise_for_status()  # Raise an error for bad responses\n",
    "#         return response.json().get(\"response\", \"No response from Phi3 model.\")\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         return f\"Error querying Phi3 model: {e}\"\n",
    "\n",
    "def query_phi3(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"  # Ensure this is the correct endpoint\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": \"phi3\", \"prompt\": prompt}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Handle HTTP errors\n",
    "        resp = \"[\"\n",
    "        # print(\"Raw Response:\", response.text) # Log the raw response for debugging)\n",
    "        for i in response:\n",
    "            resp += i.decode('utf-8').replace(\"\\n\", \"\").replace(\"}\", \"},\")\n",
    "        resp = resp[:-1] + \"]\"\n",
    "        resp = json.loads(resp)\n",
    "        ans = \"\"\n",
    "        for i in resp:\n",
    "            ans += i[\"response\"]\n",
    "        return ans\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error querying Phi3 model: {e}\"\n",
    "    except ValueError as e:\n",
    "        return f\"Error parsing JSON response: {e}\"\n",
    "\n",
    "\n",
    "# Function to perform a web search\n",
    "def web_search(question, num_results=3):\n",
    "    results = search(question, num_results=num_results)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "# RAG system function\n",
    "def answer_question_rag(question):\n",
    "    # Retrieve context from the dataset\n",
    "    context = retrieve_context(question)\n",
    "    print(\"The context retrieved is: \", context)\n",
    "    if context:\n",
    "        full_context = \"\\n\".join(context)\n",
    "        prompt = f\"Using the following IPL dataset context, answer the question:\\n\\n{full_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        response = query_phi3(prompt)\n",
    "        if response.strip().lower() not in [\"i don't know\", \"not found\", \"\"]:\n",
    "            return response\n",
    "    return None\n",
    "\n",
    "# CRAG system function (with web search fallback)\n",
    "def answer_question_crag(question):\n",
    "    # Try to answer using the RAG system\n",
    "    rag_response = answer_question_rag(question)\n",
    "    if rag_response:\n",
    "        return rag_response\n",
    "    \n",
    "    # If RAG fails, use web search and ask Phi3\n",
    "    web_results = web_search(question)\n",
    "    if web_results:\n",
    "        prompt = f\"Using the following web search results, answer the question:\\n\\n{web_results}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        return query_phi3(prompt)\n",
    "    \n",
    "    return \"I'm sorry, I couldn't find the answer to your question.\"\n",
    "\n",
    "question = \"In what locations did kolkatta knight riders and chennai super kings play a match in the year 2010?\"\n",
    "\n",
    "# RAG system response\n",
    "print(\"Using RAG system...\")\n",
    "rag_answer = answer_question_rag(question)\n",
    "print(\"RAG Answer:\", rag_answer if rag_answer else \"No answer found in the dataset.\")\n",
    "\n",
    "# CRAG system response\n",
    "# print(\"\\nUsing CRAG system...\")\n",
    "# crag_answer = answer_question_crag(question)\n",
    "# print(\"CRAG Answer:\", crag_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0529b528-c1c8-4847-887c-6b4b342e0ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.58.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.3.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai sentence-transformers pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d8fab6-a473-469b-95d4-6314c10d794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/zljzcmy576z0mwdk28nckjxw0000gn/T/ipykernel_76578/2176884651.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HuggingFaceEmbeddings.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(ipl_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Use FAISS to create a vector store from the embeddings\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformerEmbeddings(model)\n\u001b[1;32m     37\u001b[0m faiss_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(ipl_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), embedding_model)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Create a retriever to fetch relevant context based on the query\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     emit_warning()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: HuggingFaceEmbeddings.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load IPL data\n",
    "ipl_data = pd.read_csv(\"matches.csv\")\n",
    "\n",
    "# Mapping month numbers to month names\n",
    "month = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "months = {i+1: month[i] for i in range(len(month))}\n",
    "\n",
    "# Apply context generation\n",
    "ipl_data['context'] = ipl_data.apply(\n",
    "    lambda x: f\"Season: {x['season']}, Date: {x['date']} or {x['date'].split('/')[0]} of {months[int(x['date'].split('/')[1])]} {x['date'].split('/')[2]}, \"\n",
    "              f\"City: {x['city']}, Match: {x['team1']} vs {x['team2']}, Toss Winner: {x['toss_winner']}, Decision: {x['toss_decision']}, Result: {x['result']}, \"\n",
    "              f\"Duckworth-Lewis-Stern applied: {x['dl_applied']}, Winner: {x['winner']}, Win By Runs: {x['win_by_runs']}, Win By Wickets: {x['win_by_wickets']}, \"\n",
    "              f\"Player of the Match: {x['player_of_match']}, Venue: {x['venue']}, Umpire1: {x['umpire1']}, Umpire2: {x['umpire2']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert contexts into embeddings\n",
    "embeddings = model.encode(ipl_data['context'].tolist())\n",
    "\n",
    "# Use FAISS to create a vector store from the embeddings\n",
    "embedding_model = SentenceTransformerEmbeddings(model)\n",
    "faiss_store = FAISS.from_documents(ipl_data['context'].tolist(), embedding_model)\n",
    "\n",
    "# Create a retriever to fetch relevant context based on the query\n",
    "retriever = faiss_store.as_retriever()\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Create a prompt template for generating the answer\n",
    "template = \"\"\"You are a helpful assistant. Given the following IPL dataset context, answer the question.\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "# Initialize the LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Define function to retrieve relevant context and generate an answer\n",
    "def answer_question_langchain(question):\n",
    "    # Use retriever to fetch relevant contexts\n",
    "    context = retriever.retrieve(question)\n",
    "    \n",
    "    # If context found, use LLM chain to generate the answer\n",
    "    if context:\n",
    "        full_context = \"\\n\".join(context)\n",
    "        response = llm_chain.run(context=full_context, question=question)\n",
    "        return response\n",
    "    return \"No relevant context found.\"\n",
    "\n",
    "# Define the function to query Phi3 (if needed)\n",
    "def query_phi3(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"  # Ensure this is the correct endpoint\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": \"phi3\", \"prompt\": prompt}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Handle HTTP errors\n",
    "        resp = \"[\"\n",
    "        for i in response:\n",
    "            resp += i.decode('utf-8').replace(\"\\n\", \"\").replace(\"}\", \"},\")\n",
    "        resp = resp[:-1] + \"]\"\n",
    "        resp = json.loads(resp)\n",
    "        ans = \"\"\n",
    "        for i in resp:\n",
    "            ans += i[\"response\"]\n",
    "        return ans\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error querying Phi3 model: {e}\"\n",
    "    except ValueError as e:\n",
    "        return f\"Error parsing JSON response: {e}\"\n",
    "\n",
    "# Example usage\n",
    "question = \"In what locations did Kolkata Knight Riders and Chennai Super Kings play a match in the year 2010?\"\n",
    "answer = answer_question_langchain(question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f69256-c447-4b22-8618-c1830651bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.3.1)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain sentence-transformers faiss-cpu requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ac568e-63b1-4f0e-bab8-ca5c3942d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Langchain RAG system...\n",
      "\n",
      "Answer: Yes, Chennai Super Kings (CSK) played against Royal Challengers Bangalore (RCB) on April 22nd, 2017. However, the context provided does not contain information about any other match between CSK and RCB in that particular season of 2017.\n",
      "\n",
      "Source Documents Used:\n",
      "\n",
      "Document 1:\n",
      "Season: 2017, Date: 18/04/17 or 18 of april 17, City: Rajkot, Match: Royal Challengers Bangalore vs Gujarat Lions, Toss Winner: Gujarat Lions, Decision: field, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Royal Challengers Bangalore, Win By Runs: 21, Win By Wickets: 0, Player of the Match: CH Gayle, Venue: Saurashtra Cricket Association Stadium, Umpire1: S Ravi, Umpire2: VK Sharma\n",
      "\n",
      "Document 2:\n",
      "Season: 2017, Date: 27/04/17 or 27 of april 17, City: Bangalore, Match: Royal Challengers Bangalore vs Gujarat Lions, Toss Winner: Gujarat Lions, Decision: field, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Gujarat Lions, Win By Runs: 0, Win By Wickets: 7, Player of the Match: AJ Tye, Venue: M Chinnaswamy Stadium, Umpire1: AK Chaudhary, Umpire2: C Shamshuddin\n",
      "\n",
      "Document 3:\n",
      "Season: 2015, Date: 22/04/15 or 22 of april 15, City: Bangalore, Match: Chennai Super Kings vs Royal Challengers Bangalore, Toss Winner: Royal Challengers Bangalore, Decision: field, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Chennai Super Kings, Win By Runs: 27, Win By Wickets: 0, Player of the Match: SK Raina, Venue: M Chinnaswamy Stadium, Umpire1: JD Cloete, Umpire2: C Shamshuddin\n",
      "\n",
      "Document 4:\n",
      "Season: 2017, Date: 14/05/17 or 14 of may 17, City: Delhi, Match: Royal Challengers Bangalore vs Delhi Daredevils, Toss Winner: Royal Challengers Bangalore, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Royal Challengers Bangalore, Win By Runs: 10, Win By Wickets: 0, Player of the Match: HV Patel, Venue: Feroz Shah Kotla, Umpire1: CK Nandan, Umpire2: C Shamshuddin\n",
      "\n",
      "Document 5:\n",
      "Season: 2017, Date: 8/4/2017 or 8 of april 2017, City: Bangalore, Match: Royal Challengers Bangalore vs Delhi Daredevils, Toss Winner: Royal Challengers Bangalore, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Royal Challengers Bangalore, Win By Runs: 15, Win By Wickets: 0, Player of the Match: KM Jadhav, Venue: M Chinnaswamy Stadium, Umpire1: nan, Umpire2: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "ipl_data = pd.read_csv(\"matches.csv\")\n",
    "\n",
    "month = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \n",
    "         \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "months = {i+1: month[i] for i in range(len(month))}\n",
    "\n",
    "def create_context(row):\n",
    "    return (\n",
    "        f\"Season: {row['season']}, Date: {row['date']} or {row['date'].split('/')[0]} of \"\n",
    "        f\"{months[int(row['date'].split('/')[1])]} {row['date'].split('/')[2]}, \"\n",
    "        f\"City: {row['city']}, Match: {row['team1']} vs {row['team2']}, \"\n",
    "        f\"Toss Winner: {row['toss_winner']}, Decision: {row['toss_decision']}, \"\n",
    "        f\"Result: {row['result']}, Duckworth-Lewis-Stern applied: {row['dl_applied']}, \"\n",
    "        f\"Winner: {row['winner']}, Win By Runs: {row['win_by_runs']}, \"\n",
    "        f\"Win By Wickets: {row['win_by_wickets']}, Player of the Match: {row['player_of_match']}, \"\n",
    "        f\"Venue: {row['venue']}, Umpire1: {row['umpire1']}, Umpire2: {row['umpire2']}\"\n",
    "    )\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=create_context(row), metadata={\"index\": idx})\n",
    "    for idx, row in ipl_data.iterrows()\n",
    "]\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "llm = ChatOllama(model=\"phi3\", temperature=0)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "def answer_question_rag(question: str):\n",
    "    \"\"\"\n",
    "    Answer questions using the RAG system\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = qa_chain({\"query\": question})\n",
    "        return {\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"source_documents\": [doc.page_content for doc in result[\"source_documents\"]]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG system: {e}\")\n",
    "        return {\n",
    "            \"answer\": \"I'm sorry, I encountered an error while processing your question.\",\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"\"\n",
    "    \n",
    "    print(\"Using Langchain RAG system...\")\n",
    "    result = answer_question_rag(question)\n",
    "    \n",
    "    print(\"\\nAnswer:\", result[\"answer\"])\n",
    "    print(\"\\nSource Documents Used:\")\n",
    "    for idx, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        print(f\"\\nDocument {idx}:\")\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd196de-26d1-4c5e-a758-cffba8b7338d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
