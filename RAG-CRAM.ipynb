{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d80e1691-c532-4f8d-99f9-21d75d21bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement googlesearch (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for googlesearch\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install googlesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d7c922-c874-4bd7-a209-6ed92200c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAG system...\n",
      "The context retrieved is:  ['Season: 2010, Date: 4/4/2010 or 4 of april 2010, City: Kolkata, Match: Kolkata Knight Riders vs Kings XI Punjab, Toss Winner: Kolkata Knight Riders, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Kings XI Punjab, Win By Runs: 0, Win By Wickets: 8, Player of the Match: DPMD Jayawardene, Venue: Eden Gardens, Umpire1: S Asnani, Umpire2: DJ Harper', 'Season: 2010, Date: 21/03/10 or 21 of march 10, City: Chennai, Match: Kings XI Punjab vs Chennai Super Kings, Toss Winner: Chennai Super Kings, Decision: field, Result: tie, Duckworth-Lewis-Stern applied: 0, Winner: Kings XI Punjab, Win By Runs: 0, Win By Wickets: 0, Player of the Match: J Theron, Venue: MA Chidambaram Stadium, Chepauk, Umpire1: K Hariharan, Umpire2: DJ Harper', 'Season: 2010, Date: 27/03/10 or 27 of march 10, City: Chandigarh, Match: Kolkata Knight Riders vs Kings XI Punjab, Toss Winner: Kolkata Knight Riders, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Kolkata Knight Riders, Win By Runs: 39, Win By Wickets: 0, Player of the Match: MK Tiwary, Venue: Punjab Cricket Association Stadium, Mohali, Umpire1: BR Doctrove, Umpire2: S Ravi', 'Season: 2010, Date: 14/03/10 or 14 of march 10, City: Kolkata, Match: Royal Challengers Bangalore vs Kolkata Knight Riders, Toss Winner: Kolkata Knight Riders, Decision: field, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Kolkata Knight Riders, Win By Runs: 0, Win By Wickets: 7, Player of the Match: MK Tiwary, Venue: Eden Gardens, Umpire1: HDPK Dharmasena, Umpire2: AM Saheba', 'Season: 2010, Date: 13/04/10 or 13 of april 10, City: Chennai, Match: Kolkata Knight Riders vs Chennai Super Kings, Toss Winner: Kolkata Knight Riders, Decision: bat, Result: normal, Duckworth-Lewis-Stern applied: 0, Winner: Chennai Super Kings, Win By Runs: 0, Win By Wickets: 9, Player of the Match: R Ashwin, Venue: MA Chidambaram Stadium, Chepauk, Umpire1: SS Hazare, Umpire2: SJA Taufel']\n",
      "RAG Answer: In the year 2010, Kolkata Knight Riders (KKR) played matches at Eden Gardens, Chennai; Punjab Cricket Association Stadium, Mohali; and MA Chidambaram Stadium, Chepauk. On the other hand, Chennai Super Kings (CSK) had their games in Kolkata's Eden Gardens and also played at MA Chidambaram Stadium, Chepauk, which is located within city limits but serves as a venue for matches on this context when CSK was playing against KKR.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import json\n",
    "#from googlesearch import search  # For web search in CRAG\n",
    "\n",
    "# Load IPL dataset\n",
    "ipl_data = pd.read_csv(\"matches.csv\")\n",
    "\n",
    "month = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "months = {}\n",
    "\n",
    "for i in range(len(month)):\n",
    "    months[i+1] = month[i]\n",
    "# print(months)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Combine dataset columns for retrieval\n",
    "ipl_data['context'] = ipl_data.apply(\n",
    "    lambda x: f\"Season: {x['season']}, Date: {x['date']} or {x['date'].split('/')[0]} of {months[int(x['date'].split('/')[1])]} {x['date'].split('/')[2]}, City: {x['city']}, Match: {x['team1']} vs {x['team2']}, \"\n",
    "              f\"Toss Winner: {x['toss_winner']}, Decision: {x['toss_decision']}, Result: {x['result']}, Duckworth-Lewis-Stern applied: {x['dl_applied']}, \"\n",
    "              f\"Winner: {x['winner']}, Win By Runs: {x['win_by_runs']}, Win By Wickets: {x['win_by_wickets']}, Player of the Match: {x['player_of_match']}, Venue: {x['venue']}, Umpire1: {x['umpire1']}, Umpire2: {x['umpire2']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization for retrieval\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = vectorizer.fit_transform(ipl_data['context'])\n",
    "tfidf_matrix = model.encode(ipl_data['context'])\n",
    "\n",
    "# Function to retrieve relevant context\n",
    "def retrieve_context(question, k=5):\n",
    "    question_vec = model.encode([question])\n",
    "    scores = cosine_similarity(question_vec, tfidf_matrix).flatten()\n",
    "    top_indices = scores.argsort()[-k:][::-1]\n",
    "    return ipl_data.iloc[top_indices]['context'].tolist()\n",
    "\n",
    "# Function to query Phi3 via Ollama API\n",
    "def query_phi3(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"  # Replace with your server URL if using ngrok\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": \"phi3\", \"prompt\": prompt}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"response\"]\n",
    "    else:\n",
    "        return \"Error querying Phi3 model.\"\n",
    "\n",
    "def query_phi3(prompt):\n",
    "    url = \"http://localhost:11434/api/generate/\"  # Replace with ngrok URL if applicable\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": \"phi3\", \"prompt\": prompt}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        return response.json().get(\"response\", \"No response from Phi3 model.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error querying Phi3 model: {e}\"\n",
    "\n",
    "def query_phi3(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"  # Ensure this is the correct endpoint\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": \"phi3\", \"prompt\": prompt}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Handle HTTP errors\n",
    "        resp = \"[\"\n",
    "        # print(\"Raw Response:\", response.text) # Log the raw response for debugging)\n",
    "        for i in response:\n",
    "            resp += i.decode('utf-8').replace(\"\\n\", \"\").replace(\"}\", \"},\")\n",
    "        resp = resp[:-1] + \"]\"\n",
    "        resp = json.loads(resp)\n",
    "        ans = \"\"\n",
    "        for i in resp:\n",
    "            ans += i[\"response\"]\n",
    "        return ans\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error querying Phi3 model: {e}\"\n",
    "    except ValueError as e:\n",
    "        return f\"Error parsing JSON response: {e}\"\n",
    "\n",
    "\n",
    "# Function to perform a web search\n",
    "def web_search(question, num_results=3):\n",
    "    results = search(question, num_results=num_results)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "# RAG system function\n",
    "def answer_question_rag(question):\n",
    "    # Retrieve context from the dataset\n",
    "    context = retrieve_context(question)\n",
    "    print(\"The context retrieved is: \", context)\n",
    "    if context:\n",
    "        full_context = \"\\n\".join(context)\n",
    "        prompt = f\"Using the following IPL dataset context, answer the question:\\n\\n{full_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        response = query_phi3(prompt)\n",
    "        if response.strip().lower() not in [\"i don't know\", \"not found\", \"\"]:\n",
    "            return response\n",
    "    return None\n",
    "\n",
    "# CRAG system function (with web search fallback)\n",
    "def answer_question_crag(question):\n",
    "    # Try to answer using the RAG system\n",
    "    rag_response = answer_question_rag(question)\n",
    "    if rag_response:\n",
    "        return rag_response\n",
    "    \n",
    "    # If RAG fails, use web search and ask Phi3\n",
    "    web_results = web_search(question)\n",
    "    if web_results:\n",
    "        prompt = f\"Using the following web search results, answer the question:\\n\\n{web_results}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        return query_phi3(prompt)\n",
    "    \n",
    "    return \"I'm sorry, I couldn't find the answer to your question.\"\n",
    "\n",
    "question = \"In what locations did KKR and CSK play a match in the year 2010 ?\"\n",
    "\n",
    "# RAG system response\n",
    "print(\"Using RAG system...\")\n",
    "rag_answer = answer_question_rag(question)\n",
    "print(\"RAG Answer:\", rag_answer if rag_answer else \"No answer found in the dataset.\")\n",
    "\n",
    "# CRAG system response\n",
    "# print(\"\\nUsing CRAG system...\")\n",
    "# crag_answer = answer_question_crag(question)\n",
    "# print(\"CRAG Answer:\", crag_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0529b528-c1c8-4847-887c-6b4b342e0ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsrk04           24757  33.6  0.0 34403536   3260 s011  Ss+  12:18PM   0:00.24 /bin/zsh -c ps aux | grep ollama\n",
      "tsrk04           24760   0.0  0.0 33734900   1972 s011  U+   12:18PM   0:00.00 grep ollama\n"
     ]
    }
   ],
   "source": [
    "!ps aux | grep ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16d8fab6-a473-469b-95d4-6314c10d794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/12/30 12:19:00 routes.go:1259: INFO server config env=\"map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/tsrk04/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2024-12-30T12:19:00.750+05:30 level=INFO source=images.go:757 msg=\"total blobs: 5\"\n",
      "time=2024-12-30T12:19:00.750+05:30 level=INFO source=images.go:764 msg=\"total unused blobs removed: 0\"\n",
      "[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n",
      "\n",
      "[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n",
      " - using env:\texport GIN_MODE=release\n",
      " - using code:\tgin.SetMode(gin.ReleaseMode)\n",
      "\n",
      "[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n",
      "[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n",
      "[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n",
      "[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n",
      "[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n",
      "[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n",
      "[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n",
      "[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n",
      "[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\n",
      "[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n",
      "[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n",
      "[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n",
      "[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n",
      "[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n",
      "[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n",
      "time=2024-12-30T12:19:00.753+05:30 level=INFO source=routes.go:1310 msg=\"Listening on 127.0.0.1:11434 (version 0.5.4)\"\n",
      "time=2024-12-30T12:19:00.755+05:30 level=INFO source=routes.go:1339 msg=\"Dynamic LLM libraries\" runners=[cpu]\n",
      "time=2024-12-30T12:19:00.757+05:30 level=INFO source=types.go:131 msg=\"inference compute\" id=\"\" library=cpu variant=\"no vector extensions\" compute=\"\" driver=0.0 name=\"\" total=\"8.0 GiB\" available=\"1.4 GiB\"\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32f3b93b-3b02-444e-83a1-0352a144716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           ID              SIZE      MODIFIED     \n",
      "phi3:latest    4f2222927938    2.2 GB    17 hours ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6e3adf-4d28-427e-a63b-93d4b9f434ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:49.796865Z\",\"response\":\"Hello\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:49.853353Z\",\"response\":\"!\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:49.908247Z\",\"response\":\" How\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:49.960808Z\",\"response\":\" can\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:50.014812Z\",\"response\":\" I\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:50.090084Z\",\"response\":\" help\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:50.153812Z\",\"response\":\" you\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:50.221612Z\",\"response\":\" today\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:50.290942Z\",\"response\":\"?\",\"done\":false}\n",
      "{\"model\":\"phi3\",\"created_at\":\"2024-12-30T06:50:50.367752Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[32010,29871,13,10994,32007,29871,13,32001,29871,13,10994,29991,1128,508,306,1371,366,9826,29973],\"total_duration\":5281624458,\"load_duration\":540149417,\"prompt_eval_count\":10,\"prompt_eval_duration\":4157000000,\"eval_count\":10,\"eval_duration\":576000000}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:11434/api/generate -d '{\"model\": \"phi3\", \"prompt\": \"Hello\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9054615-130a-4fd7-b17a-d27b61640ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
